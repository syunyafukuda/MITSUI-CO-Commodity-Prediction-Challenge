{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80d0a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:34:06.293908Z",
     "iopub.status.busy": "2025-08-16T15:34:06.293628Z",
     "iopub.status.idle": "2025-08-16T15:34:36.469325Z",
     "shell.execute_reply": "2025-08-16T15:34:36.468448Z"
    },
    "papermill": {
     "duration": 30.180347,
     "end_time": "2025-08-16T15:34:36.470897",
     "exception": false,
     "start_time": "2025-08-16T15:34:06.290550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================== #\n",
    "# By-Price LGBM: SUBMIT (Stage-B) #\n",
    "# - price models -> log-returns   #\n",
    "# - strict output format          #\n",
    "# - seed store with exact lag dates\n",
    "# =============================== #\n",
    "\n",
    "import os, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "DATA_PATH = \"/kaggle/input/mitsui-commodity-prediction-challenge\"\n",
    "MODEL_INPUT_DIR = \"/kaggle/input/mitsui-byprice-lgbm-v1/models_price\"  # ←あなたのDataset名に合わせる\n",
    "\n",
    "# ---------------- Utils ----------------\n",
    "def preprocess_for_lgbm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj = df.select_dtypes(include=[\"object\"]).columns\n",
    "    if len(obj) > 0:\n",
    "        df[obj] = df[obj].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    for c in df.select_dtypes(include=[\"category\"]).columns:\n",
    "        df[c] = df[c].cat.codes\n",
    "    return df\n",
    "\n",
    "def parse_pair_string(s: str):\n",
    "    s = str(s)\n",
    "    if \" - \" in s:\n",
    "        a, b = s.split(\" - \", 1)\n",
    "        return a.strip(), b.strip()\n",
    "    return s.strip(), None\n",
    "\n",
    "# ---------------- Load meta & models ----------------\n",
    "with open(os.path.join(MODEL_INPUT_DIR, \"meta_price.json\"), \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "feat_cols_base = meta[\"feat_cols_base\"]\n",
    "price_cols     = meta[\"price_cols\"]\n",
    "use_log_price  = bool(meta.get(\"use_log_price\", True))\n",
    "max_lag        = int(meta.get(\"max_lag\", 7))\n",
    "\n",
    "price_models: dict[str, lgb.Booster] = {}\n",
    "for pcol in price_cols:\n",
    "    path = os.path.join(MODEL_INPUT_DIR, f\"price__{pcol}.txt\")\n",
    "    if os.path.exists(path):\n",
    "        price_models[pcol] = lgb.Booster(model_file=path)\n",
    "\n",
    "# ---------------- Labels & target map ----------------\n",
    "labels = pd.read_csv(f\"{DATA_PATH}/train_labels.csv\")\n",
    "label_cols = [c for c in labels.columns if c != \"date_id\"]\n",
    "assert len(label_cols) == (labels.shape[1] - 1)\n",
    "\n",
    "pairs = pd.read_csv(f\"{DATA_PATH}/target_pairs.csv\")\n",
    "target_map = {}\n",
    "for _, row in pairs.iterrows():\n",
    "    a, b = parse_pair_string(row[\"pair\"])\n",
    "    target_map[str(row[\"target\"])] = (a, b, int(row[\"lag\"]))\n",
    "\n",
    "# ---------------- Rolling price store ----------------\n",
    "from collections import deque, defaultdict\n",
    "class RollingPriceStore:\n",
    "    def __init__(self, max_lag: int):\n",
    "        self.max_lag = max_lag\n",
    "        self.buf = defaultdict(deque)  # price_col -> deque[(date_id, price)]\n",
    "    def push(self, price_col: str, date_id: int, price: float):\n",
    "        dq = self.buf[price_col]\n",
    "        if dq and dq[-1][0] == date_id:\n",
    "            dq[-1] = (date_id, price)\n",
    "        else:\n",
    "            dq.append((date_id, price))\n",
    "            while len(dq) > self.max_lag + 64:\n",
    "                dq.popleft()\n",
    "    def lookup(self, price_col: str, date_id: int):\n",
    "        dq = self.buf.get(price_col, None)\n",
    "        if not dq: return None\n",
    "        # exact match のみ（ここは exact でOK。代わりに事前に exact seed を入れる）\n",
    "        for d,p in reversed(dq):\n",
    "            if d == date_id:\n",
    "                return p\n",
    "        return None\n",
    "    def lookup_lag(self, price_col: str, curr_date: int, L: int):\n",
    "        return self.lookup(price_col, curr_date - L)\n",
    "\n",
    "store = RollingPriceStore(max_lag=max_lag)\n",
    "\n",
    "# ---------- NEW: exact lag seeding using train tail ----------\n",
    "# 1) train を読み込み、price_cols の最後の実測値を取得\n",
    "train_path = os.path.join(DATA_PATH, \"train.csv\")\n",
    "usecols = [\"date_id\"] + [c for c in price_cols if c not in (\"date_id\",)]\n",
    "train_all = pd.read_csv(train_path, usecols=usecols)\n",
    "train_all = preprocess_for_lgbm(train_all)\n",
    "train_all = train_all.sort_values(\"date_id\").reset_index(drop=True)\n",
    "last_train_date = int(train_all[\"date_id\"].iloc[-1])\n",
    "\n",
    "# 2) test の最初の date_id（= 最初に predict が呼ばれる想定日）を取得\n",
    "test_head = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"), usecols=[\"date_id\"]).sort_values(\"date_id\")\n",
    "first_test_date = int(test_head[\"date_id\"].iloc[0])\n",
    "\n",
    "# 3) 各 pcol について「最後の正の実測値」を取得\n",
    "last_val_per_col = {}\n",
    "for pcol in price_cols:\n",
    "    if pcol in train_all.columns:\n",
    "        v = pd.to_numeric(train_all[pcol], errors=\"coerce\").dropna()\n",
    "        v = v[v > 0]\n",
    "        if len(v) > 0:\n",
    "            last_val_per_col[pcol] = float(v.iloc[-1])\n",
    "\n",
    "# 4) exact な lag 日付に“同じ終値”を埋める（carry-forward seed）\n",
    "#    これで lookup_lag(pcol, first_test_date, k) が必ずヒットする\n",
    "for pcol, v in last_val_per_col.items():\n",
    "    for k in range(1, max_lag + 1):\n",
    "        store.push(pcol, first_test_date - k, v)\n",
    "\n",
    "# （オプション）train の末尾実測をそのまま入れておく（将来使うため）\n",
    "tail_dates = train_all[\"date_id\"].unique()[-(max_lag+8):]\n",
    "for d in tail_dates:\n",
    "    row = train_all[train_all[\"date_id\"] == d]\n",
    "    for pcol in price_cols:\n",
    "        if pcol in row.columns:\n",
    "            vv = pd.to_numeric(row[pcol], errors=\"coerce\").dropna()\n",
    "            if len(vv) > 0 and np.isfinite(vv.iloc[0]) and vv.iloc[0] > 0:\n",
    "                store.push(pcol, int(d), float(vv.iloc[0]))\n",
    "\n",
    "# ---------------- Output guard ----------------\n",
    "def _strict_output(out_dict: dict, label_cols: list[str]) -> pd.DataFrame:\n",
    "    vals = []\n",
    "    for c in label_cols:\n",
    "        v = out_dict.get(c, 0.0)\n",
    "        if v is None or not np.isfinite(v):\n",
    "            v = 0.0\n",
    "        vals.append(float(v))\n",
    "    arr = np.asarray(vals, dtype=\"float64\")\n",
    "    arr = np.clip(arr, -50.0, 50.0)\n",
    "    df = pd.DataFrame([arr], columns=label_cols)\n",
    "    df.index.name = None\n",
    "    df = df.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return df\n",
    "\n",
    "# ---------------- Core predict() ----------------\n",
    "def predict(test_batch: pl.DataFrame | pd.DataFrame, lag1, lag2, lag3, lag4) -> pd.DataFrame:\n",
    "    Xb_raw = test_batch.to_pandas() if isinstance(test_batch, pl.DataFrame) else test_batch\n",
    "    assert \"date_id\" in Xb_raw.columns, \"date_id が見つかりません\"\n",
    "    date_t = int(pd.to_numeric(Xb_raw[\"date_id\"], errors=\"coerce\").max())\n",
    "\n",
    "    Xb_raw = preprocess_for_lgbm(Xb_raw)\n",
    "\n",
    "    # 1) 当日価格の取得（実測優先 → 無ければモデル予測）\n",
    "    P_t = {}\n",
    "    for pcol in price_cols:\n",
    "        val = None\n",
    "        if pcol in Xb_raw.columns:\n",
    "            v = pd.to_numeric(Xb_raw[pcol], errors=\"coerce\").dropna()\n",
    "            if len(v) > 0:\n",
    "                val = float(v.mean())\n",
    "        if val is None:\n",
    "            booster = price_models.get(pcol)\n",
    "            if booster is not None:\n",
    "                feat_cols = [c for c in feat_cols_base if c != pcol and c in Xb_raw.columns]\n",
    "                if len(feat_cols) > 0:\n",
    "                    yhat = booster.predict(Xb_raw[feat_cols])\n",
    "                    m = float(np.asarray(yhat, dtype=\"float64\").mean())\n",
    "                    val = float(np.exp(m)) if use_log_price else m\n",
    "        if val is not None and np.isfinite(val) and val > 0:\n",
    "            P_t[pcol] = val\n",
    "            store.push(pcol, date_t, val)\n",
    "\n",
    "    # 2) targetごとに log-return 合成（exact lag が必ずヒットする）\n",
    "    out = {}\n",
    "    for tgt in label_cols:\n",
    "        a, b, L = target_map.get(tgt, (None, None, None))\n",
    "        if a is None or L is None:\n",
    "            out[tgt] = 0.0\n",
    "            continue\n",
    "        P_today = store.lookup(a, date_t) or P_t.get(a, None)\n",
    "        base_col = b if b else a\n",
    "        P_lag = store.lookup_lag(base_col, date_t, L)  # ← ここが必ずヒットするよう事前に exact seed 済み\n",
    "\n",
    "        if P_today is None or P_lag is None or P_today <= 0 or P_lag <= 0:\n",
    "            out[tgt] = 0.0\n",
    "        else:\n",
    "            out[tgt] = float(np.log(P_today) - np.log(P_lag))\n",
    "\n",
    "    return _strict_output(out, label_cols)\n",
    "\n",
    "# ---------------- Serve ----------------\n",
    "server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    server.serve()\n",
    "else:\n",
    "    server.run_local_gateway((DATA_PATH,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6f561f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:34:36.475736Z",
     "iopub.status.busy": "2025-08-16T15:34:36.475023Z",
     "iopub.status.idle": "2025-08-16T15:34:36.478790Z",
     "shell.execute_reply": "2025-08-16T15:34:36.478071Z"
    },
    "papermill": {
     "duration": 0.006884,
     "end_time": "2025-08-16T15:34:36.479934",
     "exception": false,
     "start_time": "2025-08-16T15:34:36.473050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pq_path  = \"/kaggle/working/submission.parquet\"\n",
    "# csv_out  = \"/kaggle/working/submission_from_parquet.csv\"\n",
    "\n",
    "# # 1) target_map が全ターゲットを網羅しているか\n",
    "# missing_tgts = [t for t in label_cols if t not in target_map]\n",
    "# print(\"target_mapに無いtarget数:\", len(missing_tgts))  # 0 が理想\n",
    "\n",
    "# # 2) ラベル本数と返却本数の突合テスト\n",
    "# _df = _strict_output({}, label_cols)\n",
    "# print(\"返却列数:\", _df.shape[1], \"(期待:\", len(label_cols), \")  OK?\", _df.shape[0]==1)\n",
    "\n",
    "# # ===== Inspect submission artifact (Parquet -> CSV for inspection) =====\n",
    "# # Run if necessary to check\n",
    "# sub_pl = pl.read_parquet(pq_path)\n",
    "# print(f\"[OK] Loaded Parquet: {pq_path} shape={sub_pl.shape}\")\n",
    "\n",
    "# # Export CSV for confirmation (not for submission)\n",
    "# sub_pl.write_csv(csv_out)\n",
    "# print(f\"[OK] Wrote CSV for inspection: {csv_out}\")\n",
    "\n",
    "# sub = sub_pl.to_pandas()\n",
    "# pd.set_option(\"display.max_columns\", 30)\n",
    "# sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13044405,
     "isSourceIdPinned": false,
     "sourceId": 94771,
     "sourceType": "competition"
    },
    {
     "datasetId": 8079993,
     "sourceId": 12780440,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.918711,
   "end_time": "2025-08-16T15:34:37.299387",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-16T15:34:02.380676",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
